# Effect of Text Preprocessing Methods {#Cleaning}

During natural language processes, there are many preprocessing methods researchers use to make text easier to analyze. In this work, we have removed all punctuation, new line spacing syntax, all non-alphanumeric symbols, stop words, numbers, converted to lowercase, and filtering minimum number of appearances. There are various rationales for using these preprocessing methods, but majority of these reasons boils down to reducing the complexity of the text [@tidytext]. However, these preprocessing methods may affect the ability to capture context of words as well as the ability to classify fake news. In this chapter, we will explore the effects of removing stop words and numbers, converting to lowercase, and filtering minimum number of appearances.

## Previous Literature on the Effect of Text Preprocessing

The effect of preprocessing textual data has been briefly explored in areas of text classification similar to fake news. In 2014, the effect of tokenization, stop word removal, lowercase conversion, and stemming were explored on emails (spam/not) and news (10 categories) in both the Turkish and English languages. Tokenization is the process of splitting the text into words or phrases based on a specified dictionary, or in this case, the tokenization process involves alphanumeric characters or just alphabetic characters. Stemming is the process of obtaining the root of a word. For instance, with stemming, the word "talk" and "talking" would be shortened to the root. Thus, "talking" becomes "talk" and are counted as the same word. Tokenization, stop word removal, and stemming algorithms are all dependent on the language of the text [@Uysal2014].

@Uysal2014 explore all 16 possible combinations of the text preprocessing methods. Feature selection was completed using the chi-squared method and only the top (most useful) features are maintained. They only utilize a Support Vector Machine classifier, and compare the different combinations based on the micro-F1 score [@Uysal2014]. The F1 score considers both precision and sensitivity measures. We can think of the F1 score as an average of precision and sensitivity, which allows us to get a more well-rounded understanding of the model performance by taking these classification metrics into consideration at the same time. The micro-F1 score is a global average of the F1 score [@leung2022]. Thus, for the news data set, @Uysal2014 utilize the true positives, false positives, and false negatives across all 10 classification groups globally. There does not appear to be much difference in the minimum and maximum micro-F1 scores for the email data sets, both in Turkish and English. However, for smaller feature sizes (10 and 20), the score appears much smaller than with larger feature sizes (1000 and 2000) with the news data set in both Turkish and English. 

With both the email and news data sets, @Uysal2014 report different combinations of preprocessing methods resulting in the minimum and maximum of the micro-F1 score for each feature size. For the feature size that produces the overall maximum score, they did perform a two-tailed paired t-test and find the minimum and maximum scores are significantly different for Turkish emails, Turkish news, English emails, and English news. However, no other formal hypothesis testing was done. The authors conclude different preprocessing methods significantly improve classification methods, and it varies based on language, type of text classification (email or news), and feature size [@Uysal2014]. This conclusion is based primarily on empirical evidence with no formal statistics. For instance, there were no formal tests that the feature size makes a difference or for interaction effects of the preprocessing methods. They also only consider a single classification algorithm.

In another study regarding the effect of text preprocessing, the role of $n$-grams, stop word removal, stemming, top feature selection based on two different weighting schemes, and text normalization were all explored in relation to their effect on classifying reviews [@Barushka2019]. The data they used related to hotel reviews. They had a data set containing positive reviews, both legitimate and fake and another containing negative reviews, both legitimate and fake, related to multiple hotels. They compared the effects of the mentioned text preprocessing methods using three classification methods: Naive Bayes (NB), Support Vector Machine (SVM), and a Neural Network (NN). They create a baseline setting, consisting of the top 2000 trigrams found via the TF-IDF weighting scheme, stop word removal, stemming, and document normalization. Document normalization is the process of adjusting the term frequencies such that the length of the document is taken into account. In this review, we will only focus on the accuracy statistic reported by @Barushka2019. For positive and negative reviews, the baseline preprocessing methods produced accuracies higher than 80\% for all classification methods. All but keeping stop words led to an increase in accuracy of less than 1% [@Barushka2019]. Interactions of these preprocessing methods were not explored, and formal testing was not conducted.

## Preprocessing Methods Explored

Stop words are typically removed during natural language processes due to the assumption of their lack of information. In works discussed in Chapter \ref{fakenewsmethods}, there were mixed results about the importance of stop words. @potthast found stop words, as part of different stylistic features, may not provide any informative data to detecting fake news, but @horne discovered stop words might be informative, especially in the article title. Logic dictates words such as "the" and "as" do not add to the overall meaning of a sentence. However, it is worth exploring how words contained within the 1,149 words in the `tidytext` package in `R` affect the meaning and context of sentences [@tidytext]. For instance, some of the words removed is "help," "member," and "problem." While in general these words may not provide much information, in political statements, it might. Additionally, removing stop words may put words into the same window, which would change how they are treated in Word2Vec, GloVe, and BERT. Filtering of stop words is the first factor we will be exploring to examine its effect on fake news classification.

The second factor we will be exploring is the filtering of numbers. In a recent study, there has been evidence people tend to remember numerical information that fit their views of the world, even if those numbers are false. Additionally, in the same study, as this numerical information is passed person-to-person, the distortion of these numbers becomes worse [@coronel]. This holds the potential for dramatic effects in political statements. Therefore, including numbers in our study on the effect of data preprocessing on text seems to be important.

Another preprocessing method commonly used in natural language processes is converting all characters to lowercase. Converting to lowercase makes sense with words at the start of the sentence. For instance, take the first sentence of this paragraph; the first word is "Another." It does not make sense to treat "Another" and "another" as different words. However, when it comes to proper nouns, this distinction might be significant. As an example, if a possessive, proper noun such as "President Trump's" is used, based on the fact we are removing punctuation, converting to lowercase would look like "president trumps." Now, suppose there is another statement using "trumps" in a manner such as "this policy trumps that one." A proper noun and a verb are going to be treated as the same word in majority of the embedding methods, with the exception of BERT (see Section \ref{bert}). Thus, it is of interest to explore the effect of converting all characters to lowercase on classification and embedding.

Lastly, another common preprocessing method is filtering out words that do not appear a minimum number of times across the collection of documents. In this work, we filter out words at a minimum appearance of 5, 30, and 60 times across all statements. We also include when no words are filtered at all; this would include situations where a word only appears in a single statement in our collection of 17,324 statements. In theory, filtering words out not meeting a specified number of appearances removes uninformative (or misspelled) words. However, we could be filtering out words that would help us distinguish between real and fake news. We explore if we are actually filtering out informative words aiding in classification.

## An Exploratory Analysis

### Methods

Following the set-up used in Chapter \ref{news5}, logistic regression, LDA, QDA, MDA, FDA, classification trees, and random forest are all classification methods explored, and 10-fold cross-validation is used. We compare the effect of classification on multiple embedding methods: PCA, Word2Vec CBoW (100, 200, 300, and 768 dimensions), Word2Vec SG (100, 200, 300, and 768 dimensions), GloVe (100, 200, 300, and 768 dimensions), and BERT. Bag of Words and TF-IDF are ignored in this chapter due to the computational issues related to the sparsity of these matrices. For each combination of classification and embedding method, the following effects are investigated:

-   stop word removal (yes/no)
-   converting to lowercase (yes/no)
-   filtering numbers (yes/no)
-   filtering minimum number of appearances (0/5/30/60 words minimum).

We are interested in only some subsets of the full $7\times 14\times 2\times 2\times 2\times 4$ factorial. We will be focusing our analysis on the following effects:

-   main effects
    -   classification method ($C$)
    -   embedding method ($E$)
    -   stop word removal ($S$)
    -   converting to lowercase ($L$)
    -   filtering numbers ($N$)
    -   filtering minimum number of appearances ($F$)
-   two-way interactions
    -   $C \times E$
    -   $C \times S$
    -   $C \times L$
    -   $C \times N$
    -   $C \times F$
    -   $E \times S$
    -   $E \times L$
    -   $E \times N$
    -   $E \times F$
-   three-way interactions
    -   $C \times E \times S$
    -   $C \times E \times L$
    -   $C \times E \times N$
    -   $C \times E \times F$

We will compare each of these based on their effect on accuracy, precision, sensitivity, and specificity.

It is important to note we lack replication in this study; we only have one observation per $C\times E\times S\times L\times N\times F$ combination. Thus, all other interactions will be contained within the error of the model. In addition, due to the large degrees of freedom for many of these interactions (error degrees of freedom = 2,387), p-values will produce misleading results. The F-ratio will provide more reliable understanding of the effects on our classification metrics and training time. We will be focusing only on F-ratios greater than 2. We will also be relying on visual exploration of the effects, which can be found in Appendix A. In all plots, the black triangle is the median, and the grey circle is the mean. We have used violin plots to explore our data. A violin plot allows us to visualize the distribution of the observations. Since we see the distribution of observations, we get to see how the variability is affected along with the means and medians. This is more insight than a typical mean plot. It is worth to note that for GloVe, Continuous Bag of Words, and Skip-Gram in 768 dimensions, there were computational issues with classification methods QDA, MDA, and FDA when a minimum appearances of 60 was required. This comes from trying to embed into more dimensions than unique words. As a result, these results are exploratory only.

### Accuracy

```{r accfact, echo=FALSE, message=FALSE, warning=FALSE, results = 'markup', cache=TRUE}
data.clean <- read.csv("data/DataCleaningR.csv")
data.clean.omit <- na.omit(data.clean)

data.clean.omit$FilterMinAppear_F <- 
  as.factor(data.clean.omit$FilterMinAppear)
names(data.clean.omit)[names(data.clean.omit) == 'Recall'] <- 'Sensitivity'
data.filter <- data.clean.omit[data.clean.omit$EmbeddingMethod != "BagOfWords",]
data.filter <- data.filter[data.filter$EmbeddingMethod != "TF-IDF",]
data.filter$ClassificationMethod <- gsub("LogisticRegression", "LogReg", data.filter$ClassificationMethod)
data.filter$ClassificationMethod <- factor(data.filter$ClassificationMethod, levels = c("LDA","QDA","MDA","FDA","LogReg","Tree","RF"))
data.filter$EmbeddingMethod <- factor(data.filter$EmbeddingMethod, levels = c("CBoW100","CBoW200","CBoW300","CBoW768","SG100","SG200","SG300","SG768","GloVe100","GloVe200","GloVe300","GloVe768","BERT","PCA"))

library(car)
library(dplyr)
library(tidyr)
library(ggplot2)

mod.fit.three.acc <- lm(Accuracy ~ ClassificationMethod*EmbeddingMethod*FilterStopwords +
                ClassificationMethod*EmbeddingMethod*ConvertLower +
                ClassificationMethod*EmbeddingMethod*FilterNumbers + 
                ClassificationMethod*EmbeddingMethod*FilterMinAppear_F, 
              data = data.filter)
ano.three.acc <- Anova(mod.fit.three.acc, type = "II")

CES <- data.filter %>% group_by(ClassificationMethod,EmbeddingMethod,FilterStopwords) %>% 
  summarize(CES.means = mean(Accuracy))
CEF <- data.filter %>% group_by(ClassificationMethod,EmbeddingMethod,FilterMinAppear_F) %>% 
  summarize(CEF.means = mean(Accuracy))
CEN <- data.filter %>% group_by(ClassificationMethod,EmbeddingMethod,FilterNumbers) %>% 
  summarize(CEN.means = mean(Accuracy))
CEL <- data.filter %>% group_by(ClassificationMethod,EmbeddingMethod,ConvertLower) %>% 
  summarize(CEL.means = mean(Accuracy))
CS <- data.filter %>% group_by(ClassificationMethod, FilterStopwords) %>%
  summarize(CS.means = mean(Accuracy))
ES <- data.filter %>% group_by(EmbeddingMethod, FilterStopwords) %>%
  summarize(ES.means = mean(Accuracy))
CF <- data.filter %>% group_by(ClassificationMethod, FilterMinAppear_F) %>%
  summarize(CF.means = mean(Accuracy))
EF <- data.filter %>% group_by(EmbeddingMethod, FilterMinAppear_F) %>%
  summarize(EF.means = mean(Accuracy))
CE <- data.filter %>% group_by(ClassificationMethod, EmbeddingMethod) %>%
  summarize(CE.means = mean(Accuracy))
C <- data.filter %>% group_by(ClassificationMethod) %>% summarize(C.means = mean(Accuracy))
E <- data.filter %>% group_by(EmbeddingMethod) %>% summarize(E.means = mean(Accuracy))
S <- data.filter %>% group_by(FilterStopwords) %>% summarize(S.means = mean(Accuracy))
FM <- data.filter %>% group_by(FilterMinAppear_F) %>% summarize(F.means = mean(Accuracy))
N <- data.filter %>% group_by(FilterNumbers) %>% summarize(N.means = mean(Accuracy))
```

```{r acc-anova, echo=F, message=FALSE, warning=FALSE, results = 'asis', cache=TRUE}
library(knitr)
library(kableExtra)
df.ano.acc <- data.frame(Effect = c("C", "E", "S", "L","N","F","C x E", "C x S", "E x S",
                                    "C x L", "E x L", "C x N", "E x N", "C x F", "E x F",
                                    "C x E x S", "C x E x L", "C x E x N", "C x E x F"), 
                 DF = ano.three.acc$Df[1:19],
                 Frat = ano.three.acc$`F value`[1:19])
kable(df.ano.acc,digits = 3,booktabs = T, col.names = c("Effect", "Degrees of Freedom", "F-ratio"), escape = F, caption = "Effects of Accuracy") %>%
  kable_styling()
```

Let us first look at the effect of the classification method, embedding method, and preprocessing methods on the accuracy of our model. Recall accuracy is the probability of correctly classifying real and fake news overall (see Equation \ref{accuracyeqn}). Results are in Table \@ref(tab:acc-anova). Among the three-way interactions, classification method, embedding method, and filtering of stop words appears to have some impact with a F-ratio of `r round(df.ano.acc$Frat[16],3)`. In Figure \@ref(fig:acc-SC), we can see for every classification method, almost every embedding method is improved in overall accuracy by not removing stop words. The only combination not improved when stop words remain is a classification tree and PCA. Most of the improvements are small and do not appear practical. Because almost all methods are improved when stop words are kept and the differences are so similar, this indicates visually no interaction between the classification method, embedding method, and filtering of stop words.

Examining Figure \@ref(fig:StopClassVi-Acc), we see including stop words (FilterStopwords = No) does increase the average accuracy rate across all classification methods. In general, however, there does not actually appear to be an interaction, at least visually, despite a F-ratio of `r round(df.ano.acc$Frat[8], 3)` for the $C \times S$ interaction since keeping stop words improves accuracy across all classification methods. The differences also do not appear to be very large. We also see logistic regression and classification trees are more skewed than other classification methods with their long tails but not significantly as the median and the mean are approximately the same. However, the long tails may be indicative of those methods not doing well (or at least as consistent) in learning the difference between real and fake news.

In Figure \@ref(fig:StopEmbVi-Acc), we can see all embedding methods benefit with stop words not being removed. The embedding methods BERT, PCA, and GloVe in all dimensions seem to be aided the most when stop words are maintained in terms of accuracy. Visually, we can only see the slightest of interactions, with the difference in accuracy between keeping and removing stop words for the Word2Vec embedding methods being smaller than the others (F-ratio = `r round(df.ano.acc$Frat[9], 3)`). We see visual evidence of PCA being slightly more skewed when stop words are maintained, indicating potential reliability issues. There is more variability, which a firm statement about PCA and maintaining stop words is difficult. If we combine this with what we saw in the three-way interaction discussed before, this makes sense because PCA and classification trees actually did better when stop words were removed. We can also see many of the distributions of the observations are slightly skewed and have more than a single mode. More than a single mode indicates there is inconsistency in how well methods are classifying real and fake news overall. For a unimodal, skewed distribution, majority of the methods are doing well, but there are a few outliers.

Classification method interacts with filtering threshold (F-ratio = `r round(df.ano.acc$Frat[14],3)`). In Figure \@ref(fig:MinClassVi-Acc), we see filtering out words appearing less than 30 times is slightly better in terms of accuracy per classification method, except for QDA. For QDA, removing words not appearing a minimum of 5 times improves accuracy slightly, and for random forests, filtering of a minimum of 30 performs the same as with 60. However, there does not appear to be any practical significance between any of the methods in regards to how accurate they are for the number of words filtered out. Classification trees perform the worst regardless of the filtering level. Filtering out words not appearing a minimum of 60 times has quite a few outliers when using logistic regression. We believe this represents the fact when removing words not appearing at least 60 times, we have fewer than 768 words left in the corpus of statements. Thus, embedding into 768 dimensions is actually making dimensionality larger in this case instead of smaller, and logistic regression is more sensitive to this issue than the other methods. 

The interaction between embedding method and removing words not appearing a minimum number of times has a F-ratio of `r round(df.ano.acc$Frat[15],3)`. In Figure \@ref(fig:MinEmbVi-Acc), BERT appears to do worse the more words we remove. There does not appear to be much difference between no filtering, filtering a minimum of 5, and filtering a minimum of 30 with BERT. Conversely, GloVe performs better the more words we filter out. In fact, across all dimensions of GloVe and with PCA, there appears to be a significant increase in mean and median accuracy the more words are filtered out. The Word2Vec methods are also increasing in accuracy but it is not as visually noticeable.

There is an effect of classification method by embedding method (F-ratio = `r round(df.ano.acc$Frat[7], 3)`) . There does not appear to be one classification method and embedding method greatly outperforming the other combinations (Figure \@ref(fig:ClassEmbVi-Acc)). One can say for certain not to use classification trees. With PCA in particular, the classification tree method is highly variable has quite of few outliers. Logistic regression with both Word2Vec methods in 768 dimension is performing inconsistently. Majority of the observations are performing similarly to the other classification methods, but there are several outliers affecting the mean.

Lastly, when we look at main effects, classification method, embedding method, filtering stop words, and filtering words out appearing less than a minimum number of times have very high F-ratios at `r round(df.ano.acc$Frat[1], 3)`, `r round(df.ano.acc$Frat[2], 3)`, `r round(df.ano.acc$Frat[3], 3)`, and `r round(df.ano.acc$Frat[6], 3)` respectively. Filtering out numbers appears to be significant with a F-ratio of `r round(df.ano.acc$Frat[5], 3)`. For the classification methods, random forest seems to perform the best, though this method does not appear to be practically different from LDA, logistic regression, MDA, and QDA (Figure \@ref(fig:acc-Class)). This is still seen in the medians. The methods of FDA and classification tree do not perform as well. As for embedding methods, we do not see many practical differences between the methods in terms of the means. BERT appears has the highest mean accuracy, followed by PCA (Figure \@ref(fig:acc-Emb)). The Word2Vec methods seem to be performing similarly, with SG300 being the best, but not practically different from BERT or PCA. GloVe under performs across all embedding dimensions.

As we saw earlier, maintaining stop words improves accuracy (Figure \@ref(fig:acc-Stop)). However, the difference is `r S$S.means[1]-S$S.means[2]`, which in the practical sense is not much of an improvement unless consumers are interested in a 1% increase. We can see the distributions of observations between filtering stop words and not are approximately the same, just shifted down when stop words are removed. In terms of filtering words not appearing a specified amount of times, a minimum appearance of 30 seems to perform best with an average of `r FM$F.means[3]`, though this is only `r FM$F.means[3]-FM$F.means[1]` higher than when we filter out no words (Figure \@ref(fig:acc-Min)). This does not seem very practical. We can see as more words are filtered out, the longer the left tails of the distributions get. This indicates as more words are removed, the more unreliable the classification gets (i.e. the more outliers we observe). Lastly, while the F-ratio seems to indicate there is an effect of filtering out numbers, the difference is only `r N$N.means[1]-N$N.means[2]` between maintaining numbers and removing them (Figure \@ref(fig:acc-Num)). This is not a practical difference. When we are comparing the distributions resulting from filtering numbers (or not), we can see more outliers occur when numbers are removed.

Overall, it appears the interaction of classification method, embedding method, and filtering stop words is the most influential on the accuracy rate. Maintaining stop words does improve the accuracy of the classification and embedding method combinations. Unlike what we saw in the previous chapter, there do appear to be some differences between the embedding methods as a main effect. BERT appears visually to be outperforming the other embedding methods when averaged across all other effects. As for classification methods, FDA and classification trees appear to perform the worst in terms of average accuracy. If a consumer or company is interested in maximizing the probability of correctly classifying real and fake news overall, they should maintain stop words, filter words out not appearing a minimum of 30 times, use the BERT embedding method, or use a random forest classification method. Out of all $C\times E\times S\times L\times N\times F$ combinations, logistic regression, PCA on the document-term matrix, including stop words, maintaining capitalization, keeping numbers, and filtering words not appearing a minimum of 60 times has a maximum accuracy of `r data.filter[which.max(data.filter$Accuracy),9]`. However, as we observed, logistic regression and PCA are both highly variable methods.

### Precision

```{r precfact, echo=FALSE, message=FALSE, results = 'markup', cache=TRUE}
mod.fit.three.prec <- lm(Precision ~ ClassificationMethod*EmbeddingMethod*FilterStopwords +
                ClassificationMethod*EmbeddingMethod*ConvertLower +
                ClassificationMethod*EmbeddingMethod*FilterNumbers + 
                ClassificationMethod*EmbeddingMethod*FilterMinAppear_F, 
              data = data.filter)
ano.three.prec <- Anova(mod.fit.three.prec, type = "II")
CES.prec <- data.filter %>% group_by(ClassificationMethod,EmbeddingMethod,FilterStopwords) %>% 
  summarize(CES.means = mean(Precision))
CEF.prec <- data.filter %>% group_by(ClassificationMethod,EmbeddingMethod,FilterMinAppear_F) %>% 
  summarize(CEF.means = mean(Precision))
CEN.prec <- data.filter %>% group_by(ClassificationMethod,EmbeddingMethod,FilterNumbers) %>% 
  summarize(CEN.means = mean(Precision))
CEL.prec <- data.filter %>% group_by(ClassificationMethod,EmbeddingMethod,ConvertLower) %>% 
  summarize(CEL.means = mean(Precision))
CS.prec <- data.filter %>% group_by(ClassificationMethod, FilterStopwords) %>%
  summarize(CS.means = mean(Precision))
ES.prec <- data.filter %>% group_by(EmbeddingMethod, FilterStopwords) %>%
  summarize(ES.means = mean(Precision))
CF.prec <- data.filter %>% group_by(ClassificationMethod, FilterMinAppear_F) %>%
  summarize(CF.means = mean(Precision))
EF.prec <- data.filter %>% group_by(EmbeddingMethod, FilterMinAppear_F) %>%
  summarize(EF.means = mean(Precision))
CN.prec <- data.filter %>% group_by(ClassificationMethod, FilterNumbers) %>%
  summarize(CN.means = mean(Precision))
 EN.prec <- data.filter %>% group_by(EmbeddingMethod, FilterNumbers) %>%
  summarize(EN.means = mean(Precision))
CE.prec <- data.filter %>% group_by(ClassificationMethod, EmbeddingMethod) %>%
  summarize(CE.means = mean(Precision))
C.prec <- data.filter %>% group_by(ClassificationMethod) %>% summarize(C.means = mean(Precision))
E.prec <- data.filter %>% group_by(EmbeddingMethod) %>% summarize(E.means = mean(Precision))
S.prec <- data.filter %>% group_by(FilterStopwords) %>% summarize(S.means = mean(Precision))
FM.prec <- data.filter %>% group_by(FilterMinAppear_F) %>% summarize(F.means = mean(Precision))
L.prec <- data.filter %>% group_by(ConvertLower) %>% summarize(L.means = mean(Precision))
```

```{r prec-anova, echo=F, message=FALSE, warning=FALSE, results = 'asis',cache=TRUE}
df.ano.prec <- data.frame(Effect = c("C", "E", "S", "L","N","F","C x E", "C x S", "E x S",
                                    "C x L", "E x L", "C x N", "E x N", "C x F", "E x F",
                                    "C x E x S", "C x E x L", "C x E x N", "C x E x F"), 
                 DF = ano.three.prec$Df[1:19],
                 Frat = ano.three.prec$`F value`[1:19])
kable(df.ano.prec,digits = 3,booktabs = T, col.names = c("Effect", "Degrees of Freedom", "F-ratio"), escape = F, caption = "Effects of Precision") %>%
  kable_styling()
```

Precision is the probability of correctly predicting real news; see Equation \ref{precisioneqn}. This value gives an indication of the precision of the predictions. If the probability is high, then more real news is being predicted as real (true positives), whereas if the probability is small, then more fake news is being predicted as real (false positives). The effects on precision can be found in Table \@ref(tab:prec-anova). With the interaction between classification method, embedding method, and filtering words not appearing a minimum number of appearances, there is an effect on precision with a F-ratio of `r round(df.ano.prec$Frat[19], 3)`. QDA noticeably improves precision for all Word2Vec methods when no words are removed from the analysis as seen in Figure \@ref(fig:prec-FC). As more words are filtered out, PCA and Random Forest appear to be performing better. GloVe across all classification methods and dimensions seems to improve the more words are filtered out.

Similarly to the accuracy metric, we see there is possibly an effect on precision from the interaction of classification, embedding, and filtering stop words with a F-ratio of `r round(df.ano.prec$Frat[16],3)`. In Figure \@ref(fig:prec-SC), precision is slightly improved when stop words are not removed from the analysis for almost classification methods. PCA and classification trees have improved average precision when stop words are removed. There is no difference between the presence of stop words and no stop words for all Word2Vec methods and random forest. We see more outliers with the Word2Vec methods in 768 dimensions.

For the two-way interactions, filtering of stop words again is interacting with classification method with a F-ratio of `r round(df.ano.prec$Frat[8], 3)` and affecting precision. We can see in Figure \@ref(fig:stopClassVi-prec), maintaining stop words improves precision, but it is only slightly. Both the means and the medians appear to visually be equal across classification methods between the levels of filtering stop words. Logistic regression seems to have the most outliers.

Filtering stop words also appears to interact with embedding method (F-ratio = `r round(df.ano.prec$Frat[9],3)`). All embedding methods, except Continuous Bag of Words in 100 dimensions, appears visually to have significant differences in the medians between removing or keeping stop words (Figure \@ref(fig:StopEmbVi-prec)). The largest improvements in precision visually appear with BERT and PCA, where both benefit from keeping stop words.

Unlike with accuracy, there is the slightest evidence of an interaction with classification method and filtering numbers with a F-ratio of `r round(df.ano.prec$Frat[12],3)` as well as embedding method and filtering numbers with a F-ratio of `r round(df.ano.prec$Frat[13],3)`. However, Figures \@ref(fig:numClassVi-prec) and \@ref(fig:numEmbVi-prec) shows the mean and median precision at each classification method and each embedding method is approximately the same for when numbers are maintained versus removed.

Filtering words not appearing a minimum number of times is interacting with classification method and embedding method individually, with F-ratios of `r round(df.ano.prec$Frat[14],3)` and `r round(df.ano.prec$Frat[15],3)` respectively. The primary driver of the interaction between filtering and classification method appears to be QDA and random forest seen in Figure \@ref(fig:MinClassVi-prec). For QDA, the best mean (and median) precision comes with no words filtered, whereas with random forest, filtering out words appearing less than 30 times or less than 60 times has the best mean (and median) precision. Filtering words appearing less than 30 times appears to produce the highest mean precision for each classification method other than QDA. The interaction between embedding method and filtering is visually explored in Figure \@ref(fig:MinEmbVi-prec). For BERT, one achieves similar mean (and median) precision for all levels of filtering except with filtering out a minimum number of 60 appearances. GloVe across all dimensions appears to have a decent increase in mean (and median) precision when more filtering occurs. PCA and Word2Vec methods do not appear to have as "large" of differences as with BERT and GloVe, but the fewer words removed with these methods, the better.

We also see evidence (F-ratio = `r round(df.ano.prec$Frat[7],3)`) of an interaction between embedding method and classification method. Unsurprisingly, for all embedding methods, classification trees provide worse precision (Figure \@ref(fig:ClassEmbVi-Prec)). For QDA, embedding methods of BERT, Continuous Bag of Words 768, and Skip-Gram 768 produce the largest average precision rate. This is interesting because recall 768 dimensions was picked for Word2Vec methods to see how well they compare to BERT, which has 768 dimensions. However, QDA across the all Word2Vec methods and dimensions has high variability, indicating it is not a reliable method with this embedding method. For all other embedding methods, random forest classification performs the best in terms of precision. Logistic regression also contains multiple outliers with the Word2Vec methods in 768 dimensions, which is not seen with other methods.

Lastly, similar to accuracy, classification method, embedding method, and filtering stop words have large F-ratios (`r round(df.ano.prec$Frat[1],3)`, `r round(df.ano.prec$Frat[2],3)`, and `r round(df.ano.prec$Frat[3],3)` respectively). With classification method (Figure \@ref(fig:prec-Class)), random forest is performing the best, but it does not seem practically different from LDA, logistic Regression, MDA, and QDA. Classification trees again perform worst in terms of precision. QDA has high variability, indicating it does not appear to actually be learning the difference between real and fake news. Logistic regression has a fairly symmetric distribution of observations, but it contains much longer tails than the rest of the classification methods due to many extreme outliers. For the embedding methods, PCA actually produces the highest precision, but this embedding method has high variability (Figure \@ref(fig:prec-Emb)). BERT does not seem practically different from PCA and also has large variability. As the number of dimensions increase, the Word2Vec methods begin to have more extreme outliers. GloVe appears to be performing the worse in terms of precision. With filtering stop words, `r S.prec$S.means[1]-S.prec$S.means[2]` is the difference in means for when stop words are maintained and when they are removed, which is again probably not practical (Figure \@ref(fig:prec-Stop)). 

Filtering words not appearing a minimum number of times and converting to lowercase also appear to have an effect on precision with F-ratios `r round(df.ano.prec$Frat[6],3)` and `r round(df.ano.prec$Frat[4],3)` respectively. The maximum precision occurs with filtering out words appearing less than 30 times, but this maximum is only `r FM.prec$F.means[3]-FM.prec$F.means[4]` higher than the minimum precision (Figure \@ref(fig:prec-Min)). When converting to lowercase, the difference in precision is even smaller at `r L.prec$L.means[1]-L.prec$L.means[2]`, with maintaining capitalization producing the highest precision (Figure \@ref(fig:prec-Low)). For both of these effects, these differences are so small that they do not appear to make any practical difference to precision. 

On the whole, the classification method, embedding method, filtering stop words, and filtering minimum appearances look to be the driving effects on precision. Including stop words seems to improve precision on average for both classification method and embedding method. The number of words to filter out based on the minimum number of appearances greatly depends on the classification method or embedding method used. QDA, Continuous Bag of Words, Skip-Gram, BERT, and PCA perform better respectively when fewer words are removed. As more words are filtered out, Random Forest and GloVe perform better. Out of all $C\times E\times S\times L\times N\times F$ combinations, QDA, Word2Vec Skip-Gram 768, including stop words, maintaining capitalization, keeping numbers, and filtering out no words has a maximum precision of `r data.filter[which.max(data.filter$Precision),10]`. This indicates this specific combination was creating more true positives than false positives. However, Skip-Gram in 768 dimensions and QDA are both respectively highly variable, as well as variable in combination as seen in Figure \@ref(fig:ClassEmbVi-Prec). 

### Sensitivity

```{r sensfact, echo=FALSE, message=FALSE, results = 'markup', cache=TRUE}
mod.fit.three.sens <- lm(Sensitivity ~ ClassificationMethod*EmbeddingMethod*FilterStopwords +
                ClassificationMethod*EmbeddingMethod*ConvertLower +
                ClassificationMethod*EmbeddingMethod*FilterNumbers + 
                ClassificationMethod*EmbeddingMethod*FilterMinAppear_F, 
              data = data.filter)
ano.three.sens <- Anova(mod.fit.three.sens, type = "II")
CES.sens <- data.filter %>% group_by(ClassificationMethod,EmbeddingMethod,FilterStopwords) %>% 
  summarize(CES.means = mean(Sensitivity))
CEF.sens <- data.filter %>% group_by(ClassificationMethod,EmbeddingMethod,FilterMinAppear_F) %>% 
  summarize(CEF.means = mean(Sensitivity))
CEN.sens <- data.filter %>% group_by(ClassificationMethod,EmbeddingMethod,FilterNumbers) %>% 
  summarize(CEN.means = mean(Sensitivity))
CEL.sens <- data.filter %>% group_by(ClassificationMethod,EmbeddingMethod,ConvertLower) %>% 
  summarize(CEL.means = mean(Sensitivity))
CS.sens <- data.filter %>% group_by(ClassificationMethod, FilterStopwords) %>%
  summarize(CS.means = mean(Sensitivity))
CF.sens <- data.filter %>% group_by(ClassificationMethod, FilterMinAppear_F) %>%
  summarize(CF.means = mean(Sensitivity))
EF.sens <- data.filter %>% group_by(EmbeddingMethod, FilterMinAppear_F) %>%
  summarize(EF.means = mean(Sensitivity))
CN.sens <- data.filter %>% group_by(ClassificationMethod, FilterNumbers) %>%
  summarize(CN.means = mean(Sensitivity))
CE.sens <- data.filter %>% group_by(ClassificationMethod, EmbeddingMethod) %>%
  summarize(CE.means = mean(Sensitivity))
C.sens <- data.filter %>% group_by(ClassificationMethod) %>% summarize(C.means = mean(Sensitivity))
E.sens <- data.filter %>% group_by(EmbeddingMethod) %>% summarize(E.means = mean(Sensitivity))
S.sens <- data.filter %>% group_by(FilterStopwords) %>% summarize(S.means = mean(Sensitivity))
L.sens <- data.filter %>% group_by(ConvertLower) %>% summarize(L.means = mean(Sensitivity))
N.sens <- data.filter %>% group_by(FilterNumbers) %>% summarize(N.means = mean(Sensitivity))
FM.sens <- data.filter %>% group_by(FilterMinAppear_F) %>% summarize(F.means = mean(Sensitivity))
```

```{r sens-anova, echo=F, message=FALSE, warning=FALSE, results = 'asis', cache=TRUE}
df.ano.sens <- data.frame(Effect = c("C", "E", "S", "L","N","F","C x E", "C x S", "E x S",
                                    "C x L", "E x L", "C x N", "E x N", "C x F", "E x F",
                                    "C x E x S", "C x E x L", "C x E x N", "C x E x F"), 
                 DF = ano.three.sens$Df[1:19],
                 Frat = ano.three.sens$`F value`[1:19])
kable(df.ano.sens,digits = 3,booktabs = T, col.names = c("Effect", "Degrees of Freedom", "F-ratio"), escape = F, caption = "Effects of Sensitivity") %>%
  kable_styling()
```

Found in Table \@ref(tab:sens-anova) are the results exploring the effect on sensitivity. Sensitivity is the probability of correctly identifying real news; see Equation \ref{recalleqn}. If this number is high, we have more true positives, where real news is being accurately classified as real. If it is low, we have more false negatives, where more real news is being classified as fake. The interaction of classification method, embedding method, and removing words not appearing a specified minimum number times looks to be impacting sensitivity (F-ratio = `r round(df.ano.sens$Frat[19],3)`). In Figure \@ref(fig:sens-FC), we notice there is a lot of variability with QDA, with average sensitivity generally improving as the number of words removed increases. The Word2Vec methods in particular do poorly when no words are removed, and QDA is used. PCA and random forest is also not performing well in terms of sensitivity.

The interaction between classification method and filtering of stop words is still creating an impact of sensitivity just as it did with accuracy and precision (F-ratio = `r round(df.ano.sens$Frat[8],3)`). Figure \@ref(fig:stopClassVi-sens) shows for majority of the classification methods, the difference in the mean (and median) sensitivity for when stop words are present and when they are removed is about the same is largest for random forest and classification trees. Maintaining stop words appears to help random forest and classification trees predict real news correctly. All other classification methods visually do not show much difference at all between keeping stop words and removing them. Logistic regression has a symmetric distribution but has long tails. QDA also seems pretty skewed given the differences in mean and median. Both of these do not seem as reliable.

Classification method and filtering numbers also appear to interact in terms of sensitivity (F-ratio = `r round(df.ano.sens$Frat[12],3)`).  However, it does not appear there is a difference between the mean (and median) sensitivity rates for when we include numbers versus when we do not for all classification methods, as seen in Figure \@ref(fig:numClassVi-sens). The "largest" difference is with QDA, where keeping numbers seems to improve the probability of correctly identifying real news. Logistic regression has a symmetric distribution with long tails for when numbers are removed. When numbers are kept, the distribution for logistic regression is no longer symmetric and has outliers performing worse than the bulk of the observations. QDA again seems pretty skewed both with and without numbers with the differences in means and medians. 

With F-ratios `r round(df.ano.sens$Frat[14],3)` and `r round(df.ano.sens$Frat[15],5)` respectively, both classification method and embedding method interact individually with filtering out words not appearing a minimum number of times. For the discriminant analyses, there are similar average sensitivities for filtering minimum of 30 and 60 appearances (Figure \@ref(fig:minClassVi-sens)). For random forest, filtering out fewer words improves mean (and median) sensitivity. Classification trees do not appear to have much difference between the numbers of words removed. We again see quite a bit of variation when using QDA, and this classification method seems to perform much better when words appearing less than 30 or less than 60 times are removed. QDA has high variability when no words are removed. Logistic regression when removing words not appearing a minimum of 60 times also has many extreme outliers. When exploring the effect of embedding method and filtering words (Figure \@ref(fig:minEmbVi-sens)), removing no words from the analysis has the lowest sensitivity rate across the board, except again with BERT. In general, filtering out words that do not appear a minimum of 30 times appears to help each embedding method outside of BERT. When removing no words, the Word2Vec methods do much worse across the board. Multiple combinations appear to have outliers, creating long tails.

The last interaction creating an impact on sensitivity is between classification method and embedding method (F-ratio = `r round(df.ano.sens$Frat[7],3)`), as we have seen with both accuracy and precision. Figure \@ref(fig:ClassEmbVi-sens) shows large variability with QDA across the Word2Vec methods. Random forest and PCA are producing the lowest average sensitivity rate. The distributions associated with classification trees have high variability. The distribution for GloVe 100/200/300 with QDA is noticeably higher than other combinations. 

Finally, we explore the main effects. In terms of sensitivity, all main effects appear to be influential. Classification method has a F-ratio of `r round(df.ano.sens$Frat[1],3)`, and in Figure \@ref(fig:sens-Class), we notice a decrease in sensitivity with classification trees. We see how variable QDA and classification trees are in comparison to other methods, and logistic regression creates many extreme outliers. The median sensitivity for QDA does visually appear to be the highest. In Figure \@ref(fig:sens-Emb), we gain insight into the embedding methods (F-ratio = `r round(df.ano.sens$Frat[2],3)`). GloVe in 100 dimensions produces the highest mean (and median) sensitivity, where as Skip-Gram in 768 dimensions is the lowest. In general, fewer dimensions produce higher sensitivity rates.

The filtering of stop words again has a large F-ratio at `r round(df.ano.sens$Frat[3],3)`. Not removing stop words improves sensitivity by `r S.sens$S.means[1]-S.sens$S.means[2]`. If we are wanting to detect real news correctly, we are better off not removing stop words. While the F-ratio of `r round(df.ano.sens$Frat[4],3)` implies there could possibly be an effect of converting to lowercase, Figure \@ref(fig:sens-Low) indicates there is no difference between maintaining capitalization and converting to lowercase. Converting letters to lowercase only increases sensitivity by `r L.sens$L.means[2]-L.sens$L.means[1]` compared to maintaining capitalization, which is not practically significant. The F-ratio of `r round(df.ano.sens$Frat[5],3)` for removing numbers also indicates a significant effect on sensitivity. However, in Figure \@ref(fig:sens-Num), there does not appear to be a difference, and maintaining numbers only increases sensitivity by `r N.sens$N.means[1]-N.sens$N.means[2]`. 

Lastly, filtering out words not appearing a minimum number of times has a F-ratio of `r round(df.ano.sens$Frat[6],3)`, indicating very strong evidence of an effect on sensitivity. Figure \@ref(fig:sens-Min) shows filtering words not appearing a minimum of 30 times performs the best, and a minimum of 60 appearances does not seem that different from 30. Filtering out no words performs much worse than the other levels. The average sensitivity when removing words not appearing 30 times is `r FM.sens$F.means[3]-FM.sens$F.means[1]` higher than when no words are removed. This indicates the more words one filters out, the better the probability of correctly classifying real news. There are, however, many extreme outliers for all levels of minimum number of appearances, which would affect the means. However, the medians seem to follow the same pattern as we see with the means.

In general, we see many of the same patterns we saw with accuracy and precision. Classification method, embedding method, filtering of stop words, and filtering out words not appearing a minimum number of times play a role in how well we are detecting real news correctly. In general, random forest and classification trees do not perform as well as other methods. To improve sensitivity, we want to keep stop words as well as increase the minimum number of appearances a word must have in order to remain in the analysis. GloVe in 100 dimensions seems to produce the highest sensitivity, but much like we saw in Chapter \ref{news5}, sensitivity is much more variable than accuracy and precision, as seen with many of the extreme outliers in many of the distributions. We really need to consider these results in conjunction with specificity. Out of all $C\times E\times S\times L\times N\times F$ combinations, Logistic Regression, Word2Vec Skip-Gram 768, including stop words, maintaining capitalization, removing numbers, and filtering out words not appearing a minimum of 60 times has a maximum sensitivity of `r data.filter[which.max(data.filter$Sensitivity),11]`. Thus, we are able to classify real news alone extremely well with this combination. However, the minimum sensitivity value is `r data.filter[which.min(data.filter$Sensitivity),11]` is for the exact same combination as the maximum except numbers are kept in the analysis. This further supports the large variability we see in sensitivity. We also need to keep in mind both logistic regression and Skip-Gram in 768 dimensions were the combination that produced the most extreme outliers.

### Specificity

```{r specfact, echo=FALSE, message=FALSE, results = 'markup', cache=TRUE}
mod.fit.three.spec <- lm(Specificity ~ ClassificationMethod*EmbeddingMethod*FilterStopwords +
                ClassificationMethod*EmbeddingMethod*ConvertLower +
                ClassificationMethod*EmbeddingMethod*FilterNumbers + 
                ClassificationMethod*EmbeddingMethod*FilterMinAppear_F, 
              data = data.filter)
ano.three.spec <- Anova(mod.fit.three.spec, type = "II")
CES.spec <- data.filter %>% group_by(ClassificationMethod,EmbeddingMethod,FilterStopwords) %>% 
  summarize(CES.means = mean(Specificity))
CEF.spec <- data.filter %>% group_by(ClassificationMethod,EmbeddingMethod,FilterMinAppear_F) %>% 
  summarize(CEF.means = mean(Specificity))
CEN.spec <- data.filter %>% group_by(ClassificationMethod,EmbeddingMethod,FilterNumbers) %>% 
  summarize(CEN.means = mean(Specificity))
CEL.spec <- data.filter %>% group_by(ClassificationMethod,EmbeddingMethod,ConvertLower) %>% 
  summarize(CEL.means = mean(Specificity))
CS.spec <- data.filter %>% group_by(ClassificationMethod, FilterStopwords) %>%
  summarize(CS.means = mean(Specificity))
ES.spec <- data.filter %>% group_by(EmbeddingMethod, FilterStopwords) %>%
  summarize(ES.means = mean(Specificity))
CN.spec <- data.filter %>% group_by(ClassificationMethod, FilterNumbers) %>%
  summarize(CN.means = mean(Specificity))
EN.spec <- data.filter %>% group_by(EmbeddingMethod, FilterNumbers) %>%
  summarize(EN.means = mean(Specificity))
CF.spec <- data.filter %>% group_by(ClassificationMethod, FilterMinAppear_F) %>%
  summarize(CF.means = mean(Specificity))
EF.spec <- data.filter %>% group_by(EmbeddingMethod, FilterMinAppear_F) %>%
  summarize(EF.means = mean(Specificity))
CE.spec <- data.filter %>% group_by(ClassificationMethod, EmbeddingMethod) %>%
  summarize(CE.means = mean(Specificity))
C.spec <- data.filter %>% group_by(ClassificationMethod) %>% summarize(C.means = mean(Specificity))
E.spec <- data.filter %>% group_by(EmbeddingMethod) %>% summarize(E.means = mean(Specificity))
S.spec <- data.filter %>% group_by(FilterStopwords) %>% summarize(S.means = mean(Specificity))
L.spec <- data.filter %>% group_by(ConvertLower) %>% summarize(L.means = mean(Specificity))
N.spec <- data.filter %>% group_by(FilterNumbers) %>% summarize(N.means = mean(Specificity))
FM.spec <- data.filter %>% group_by(FilterMinAppear_F) %>% summarize(F.means = mean(Specificity))
```

```{r spec-anova, echo=F, message=FALSE, warning=FALSE, results = 'asis',cache=TRUE}
df.ano.spec <- data.frame(Effect = c("C", "E", "S", "L","N","F","C x E", "C x S", "E x S",
                                    "C x L", "E x L", "C x N", "E x N", "C x F", "E x F",
                                    "C x E x S", "C x E x L", "C x E x N", "C x E x F"), 
                 DF = ano.three.spec$Df[1:19],
                 Frat = ano.three.spec$`F value`[1:19])
kable(df.ano.spec,digits = 3,booktabs = T, col.names = c("Effect", "Degrees of Freedom", "F-ratio"), escape = F, caption = "Effects on Specificity") %>%
  kable_styling()
```

The effects on specificity are presented in Table \@ref(tab:spec-anova). Specificity is the probability of correctly classifying fake news. If this number is large, then we have more true negatives, where we have correctly found fake news. If the number is small, then we have more false positives, where we have incorrectly identified fake news as real. See Equation \ref{specificityeqn}. We see much of the same as we saw before with the three-way interactions with sensitivity. The only interaction seemingly significant is between classification method, embedding method, and removing words not appearing a minimum number of times (F-ratio = `r round(df.ano.spec$Frat[19],3)`). If we look at Figure \@ref(fig:spec-FC), we see QDA has large variability again, especially when no words are removed. Skip-Gram in 300 dimensions with QDA clearly performs the best when no words are removed. However, as more words are removed, specificity is significantly affected for that combination. In general, random forest appears to be performing better as more words are removed, especially with PCA. If we were to put this plot side-by-side with the associated plot with sensitivity, we see an almost inverse relationship. The methods performing the worst in regards to sensitivity are performing the best in terms of specificity. This is what we saw in Chapter \ref{news5}. This inverse relationship is particularly seen in classification methods of QDA and random forest, but for other classification methods, the number of words removed performing the best across the embedding methods does flip from sensitivity.

Classification method and filtering of stop words again seem to have an effect on specificity (F-ratio = `r round(df.ano.spec$Frat[8],3)`). In Figure \@ref(fig:stopClassVi-spec), random forest and classification tree methods are actually aided by removing stop words. There does not appear to be much difference between keeping stop words and removing them for all other classification methods except for FDA. FDA appears to be aided the most by maintaining stop words. This plot does provide indication of an interaction. We do see visual evidence of the inverse relationship with sensitivity, specifically in QDA (Figure \@ref(fig:stopClassVi-sens)). For the other classification methods, the inverse relationship with sensitivity is only seen in the extreme outliers, which seemed to represent unreliable combinations anyways.

Embedding method and filtering of stop words has a F-ratio of `r round(df.ano.spec$Frat[9],3)`, but Figure \@ref(fig:stopEmbVi-spec) reveals there is not much difference between removing or keeping stop words across all embedding methods. We see multiple outliers with the Word2Vec methods in all dimensions.

Figure \@ref(fig:numClassVi-spec) shows the possible interaction between filtering numbers and classification method (F-ratio = `r round(df.ano.spec$Frat[12],3)`). There does not appear to be any differences in the mean (and median) in specificity between keeping and removing numbers for all classification methods. This graph also visually appears to be a complete mirror of the associated graph with sensitivity (Figure \@ref(fig:numClassVi-sens)). The primary drivers of this inverse relationship appears to be QDA and logistic regression. When numbers are not filtered, the outliers are now performing better than the bulk of the observations. The interaction between filtering numbers and embedding methods has a F-ratio of `r round(df.ano.spec$Frat[13],3)`, but Figure \@ref(fig:numEmbVi-spec) shows there is not much difference between keeping or removing numbers across the different embedding methods.

Filtering threshold is interacting with classification method and embedding method respectively (F-ratios `r round(df.ano.spec$Frat[14],3)` and `r round(df.ano.spec$Frat[15],3)`). We can see, in Figure \@ref(fig:minClassVi-spec), QDA again has the most variability, with no words being filtered performing the best for specificity. For random forest, it appears the more words one filters out, the better the predictive performance of fake news. The same is said with classification trees, but the difference is not as large as with random forests. For all other methods, when fewer words are filtered out, the better specificity is attained. When compared to the associated interaction plot with sensitivity (Figure \@ref(fig:minClassVi-sens)), it is a mirrored image again. In general, as more words are filtered out, the centers of the distributions are decreasing except with random forest. This is opposite of the distributions seen with sensitivity. For the embedding methods, filtering out no words appears to significantly help improve specificity for both Word2Vec methods across all dimensions (Figure \@ref(fig:minEmbVi-spec)). With GloVe, the more words removed, the better specificity is achieved. The Word2Vec methods and PCA have a mirrored relationship compared to what we saw with sensitivity (Figure \@ref(fig:minEmbVi-sens)). With sensitivity, the more words we removed, the better the performance on fake news with Word2Vec and PCA.

The last two-way interaction, classification method and embedding method, is also significant with a F-ratio of `r round(df.ano.spec$Frat[7],3)`. We again see large variability with QDA across the embedding methods in Figure \@ref(fig:ClassEmbVi-spec), except for BERT and GloVe in all dimensions. Random forest performs the best across all embedding methods in general. QDA performs better in terms of the mean (and the median) for certain embedding methods, but due to the variability, this does not seem like a reliable method. This again appears to be the opposite of sensitivity, primarily seen with QDA and the random forest/PCA combination (see Figure \@ref(fig:ClassEmbVi-sens)).

Classification method (F-ratio = `r round(df.ano.spec$Frat[1],3)`), embedding method (F-ratio = `r round(df.ano.spec$Frat[2],3)`), converting to lowercase (F-ratio = `r round(df.ano.spec$Frat[4],3)`), filtering numbers (F-ratio = `r round(df.ano.spec$Frat[5],3)`), and filtering threshold (F-ratio = `r round(df.ano.spec$Frat[6],3)`) are all significant. In Figure \@ref(fig:spec-Class), random forest appears to be improving the specificity rate. We also see high variability with both QDA and classification tree as we did with sensitivity. Logistic regression is a symmetric distribution, but it seems to produce outliers. For embedding methods, BERT is performing the best, but it does not appear significantly different from Skip-Gram in 768 dimensions. GloVe in 100 dimensions is noticeably the worst (Figure \@ref(fig:spec-Emb)). In this case, as the number of dimensions increases, so does the probability of correctly classifying fake news, which is the opposite of what we saw with sensitivity. There are again many extreme outliers with the embedding methods and specificity.

Converting to lowercase and filtering out numbers have somewhat high F-ratios, as mentioned. However, in Figures \@ref(fig:spec-Low) and \@ref(fig:spec-Num), we see the F-ratio is slightly misleading. There does not visually appear to be a difference, let alone a practical difference. Maintaining capital letters only improves specificity by `r L.spec$L.means[1]-L.spec$L.means[2]` over converting to lowercase. Removing numbers only improves specificity by `r N.spec$N.means[2]-N.spec$N.means[1]` over keeping numbers. For both of these preprocessing methods, the extreme outliers seem to be the driving influence on the inverse relationship with sensitivity. When filtering out words based on number of appearances, filtering out no words seems to offer the most improvement, and it is `r FM.spec$F.means[1]-FM.spec$F.means[2]` above a cutoff of a minimum of 5 appearances (Figure \@ref(fig:spec-Min)). The trend we see with specificity is again an approximate inverse of what we see with sensitivity, but the primary driver looks to be the extreme outliers. 

Overall, specificity seems most influenced by classification method, embedding method, and filtering out words not appearing a specified minimum number of times. QDA appears to be, again, highly variable across the different embedding methods and preprocessing methods. In general, random forest seems to be able to classify fake news correctly the best. For embedding methods, BERT and Skip-Gram in 768 dimensions appears to be the best in terms of specificity. Removing no words appears to also be helping classify fake news correctly. Unlike with other metrics, stop words only appear to be interacting with classification method, where removing stop words aids random forests and classification trees. Out of all $C\times E\times S\times L\times N\times F$ combinations, Logistic Regression, Word2Vec Skip-Gram 768, including stop words, maintaining capitalization, including numbers, and filtering out words not appearing a minimum of 60 times has a maximum specificity of `r data.filter[which.max(data.filter$Specificity),12]`. The minimum (`r data.filter[which.min(data.filter$Specificity),12]`) is again for the same combination except for with removing numbers. These combinations are exactly reversed from what we saw with sensitivity! The maximum sensitivity value occurs for the combination that has the minimum specificity value. This leads us to question if the methods are actually learning the difference between real and fake news or just picking one to place all observations, and as we saw, the extreme outliers were the primary influences on this flipped relationship. These combinations definitely fall as extreme outliers and need to be carefully considered. 

## Conclusions

Across all of the metrics calculated, classification method, embedding method, filtering of stop words, and filtering out words not appearing a minimum number of times appear to be the most influential, as well as various interactions involving these four factors. In general, classification trees do not perform well for any of the metrics, and for all but sensitivity, random forest outperforms the other classification methods. This pattern emerges across all interactions involving classification method. There are occasions when QDA outperforms random forest, but QDA does have higher variability. As for the embedding methods, no method really seemed to be outperforming the others for all four metrics. For accuracy and precision, BERT and PCA appeared to be the best. For sensitivity and specificity, GloVe and Skip-Gram appear to be the best methods. Again, this pattern recurs in the interactions involving embedding method. 

Filtering out stop words and filtering out words not appearing a minimum number of times are not individually impactful on the four metrics. The differences between levels of the respective individual factors are quite small. However, when each factor interacts with classification method and/or embedding method, we start to see how those preprocessing method affect our metrics. In general, keeping stop words in the analysis improves all four metrics. The only instance maintaining stop words was not beneficial was with random forest and classification trees in terms of the probability of correctly classifying fake news. The effect of filtering out words not appearing a minimum number of times depended quite a bit on which embedding method was used. For BERT in particular, it was aided by keeping more words in the analysis, whereas with GloVe, the more words we filtered out the better. 

In a majority of the figures related to specificity, the patterns we see are the inverse of what we saw with sensitivity, similar to what we saw in Chapter \ref{news5}. This raises the question: are the classification methods actually learning the difference between real and fake news? It looks as if the classification methods are choosing to classify as either all real or all fake and not really learning. However, this does depend on which embedding method and preprocessing methods we use. The embedding method and preprocessing methods are playing some kind of role in whether or not the classification method over predicts real news or over predicts fake news. Embedding methods and preprocessing methods correctly identifying real news do poorly when correctly identifying fake news. One of the possible explanations is the fact the original six labels in the PolitiFact data set have been condensed down to two, which could muddle the classification methods. We will explore this in the next chapter. Additionally, the inverse relationship seems to be primarily influenced by the extreme outliers. More exploration on what these outliers have in common is needed. If the outliers are primarily related to embedding methods, this could indicate the method of averaging across dimensions used to combine word embeddings into statement embeddings is inadequate. Further exploration of methods of embedding whole statements instead of just words are needed and is left for future work.

Lastly, the preprocessing method of stemming was not considered in this work. There are many stemming methods available for consideration, adding in another layer of complexity. Stemming would have some effect on all of the metrics explored here and will also be considered in a future work. 
